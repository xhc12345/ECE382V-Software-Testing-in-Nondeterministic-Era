prompts:
  test_fun_fact: |
    Tell me a short fun fact about space.
    Preferably, it's about the Solar System.

  test_json_response: |
    Compare Python, Lua, Java, C#, Golang. Respond as a JSON in the following format listed below. Do not add any decorators or comments, only give JSON response as plain text.
    {
      "winner": <language name>,
      "reason": <explain why winner is this language>
      "languages": [
        {
          "name":<name of language>,
          "description":<description of language>,
          "year_created":<which year it was created>,
          "compatibility":<how compatible is the language across different platforms>,
          "speed":<describe how fast is the langauge run>
        }
      ]
    }

  test_check_previous_conversation: |
    What was my previous question(s)?

  praise_the_LLM: |
    You are a flaky test detection tool.

  flaky_detect_1: |
    In software testing, a flaky test is a test that will output different results on different runs   despite testing the same source code.
    The given test (suite) given below is composed of one or more tests that may or may not be flaky.
    Analyze each test to determine if there are any flaky behaviors, and identify which lines have what kind of flaky behavior.
    For categorization, use to the following rules:
      OD = Order-Dependent flaky tests as defined in iDFlakies
      OD-Brit = Order-Dependent Brittle tests as defined in iFixFlakies
      OD-Vic = Order-Dependent Victim tests as defined in iFixFlakies
      ID = Implementation-Dependent Tests found by Nondex
      ID-HtF = Implementation-Dependent tests that are hard to fix.
      NIO = Non-Idempotent-Outcome Tests as defined in ICSE'22 work. Tests that pass in the first run but fail in the second.
      NOD = Non-Deterministic tests
      NDOD = Non-Deterministic Order-Dependent tests that fail non-deterministically but with significantly different failure rates in different orders as defined in our ISSRE'20 work
      NDOI = Non-Deterministic Order-Independent tests that fail non-deterministically but similar failure rates in all orders as defined in our ISSRE'20 work
      UD = Unknown Dependency tests that pass and fail in a test suite or in isolation
      OSD = Operating System Dependent tests that pass and fail depending on the operating system
      TZD = Tests that fail in machines on different time zones, usually failing time-related assertions
    List the flakies that aren't flaky.
    For the ones that are flaky, state the flaky test name, and the code that led to the flaky behaviors.
    Categorize each one based on the categorization rules above.
    Explain why each flaky behavior is flaky, 1-3 sentences.

  jsonify_prev_1: |
    Convert your previous response to a JSON in the following format listed below. Do not add any decorators or comments, only give JSON response as plain text, do not wrap it in code blocks.
    {
      "flakies": [
        {
          "name":<name of the test that is flaky>,
          "flakyOcurrences":[
            {
              "lineNumber": <string of from which line to which line does this flaky behavior occurred, e.g. "4-7">,
              "lineCode": <valid JSON string of the code that caused this flaky behavior>,
              "Category": <abbreviation of the categorization best fit for this test>,
              "explain": <1-3 sentences explain why this is flaky>
            }
          ]
        }
      ],
      "notFlakies": [
        <name of the test that is NOT flaky>,
      ]
    }

  jsonify_detect_1: |
    Given this JSON format:
    {
      "flakies": [
        {
          "name":<name of the test that is flaky>,
          "flakyOcurrences":[
            {
              "lineNumber": <string of from which line to which line does this flaky behavior occurred, e.g. "4-7">,
              "lineCode": <valid JSON string of the code that caused this flaky behavior>,
              "Category": <abbreviation of the categorization best fit for this test>,
              "explain": <1-3 sentences explain why this is flaky>
            }
          ]
        }
      ],
      "notFlakies": [
        <name of the test that is NOT flaky>,
      ]
    }
    Convert each of the 3 LLM (ChatGPT, Gemini, Llama) responses below into the above JSON format exactly as they are, do not modify their responses (Do not change their answer in Category).
    Then respond with a JSON object of the format:
    {
      "ChatGPT": <LLM response in given JSON format>,
      "Gemini": <LLM response in given JSON format>,
      "Llama": <LLM response in given JSON format>
    }
    Do not add any decorators or comments, only give JSON response as plain text, do not wrap it in code blocks.
    Responses to convert:

  flaky_detect_2: |
    For the previous question, I also asked 2 other large langauge models (LLM) for their analysis with the same prompt. Their responses are at the end of this prompt.
    Compare your response to their responses and fix your mistakes or point out if they analyzed incorrectly.
    Once you made changes (if any) to your previous response, do the following:
    State the test names that are not flaky.
    For the ones that are flaky, state the flaky test name, and the code that led to the flaky behaviors.
    Name of all the LLM models that flagged this test with the correct Category in the previous response. If your response does not change compared to your last response, include your model name too.
    Categorize each one based on the categorization rules above.
    Explain why each flaky behavior is flaky, 1-3 sentences.
    
  jsonify_detect_2: |
    Given this JSON format:
    {
      "flakies": [
        {
          "name":<name of the test that is flaky>,
          "correctlyAnalyzed":[<list of LLM model names that got this right>],
          "flakyOcurrences":[
            {
              "lineNumber": <string of from which line to which line does this flaky behavior occurred, e.g. "4-7">,
              "lineCode": <valid JSON string of the code that caused this flaky behavior>,
              "Category": <abbreviation of the categorization best fit for this test>,
              "explain": <1-3 sentences explain why this is flaky>
            }
          ]
        }
      ],
      "notFlakies": [
        <name of the test that is NOT flaky>,
      ]
    }
    Convert each of the 3 LLM (ChatGPT, Gemini, Llama) responses below into the above JSON format exactly as they are, do not modify their responses (Do not change their answer in Category).
    Then respond with a JSON object of the format:
    {
      "ChatGPT": <LLM response in given JSON format>,
      "Gemini": <LLM response in given JSON format>,
      "Llama": <LLM response in given JSON format>
    }
    Do not add any decorators or comments, only give JSON response as plain text, do not wrap it in code blocks.
    Responses to convert:

  prompt2: |
    This is the second prompt.
    Remember to include all important details here, and feel free
    to use line breaks for readability.

  prompt3: |
    This is the third prompt.
    You can even include special characters like "quotes", $dollar signs,
    and line breaks to format your message effectively.
